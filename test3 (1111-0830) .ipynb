{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"./\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "import skimage.io\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_MAX_DIM                  512\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  512\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [512 512   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           violin\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                23.625\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           80\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               30\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ViolinConfig(Config):\n",
    "\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"violin\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # background + 1 shape\n",
    "\n",
    "    IMAGE_MIN_DIM = 512 #changed\n",
    "    IMAGE_MAX_DIM = 512 # changed\n",
    "\n",
    "    #RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "\n",
    "    TRAIN_ROIS_PER_IMAGE = 80 ## changed\n",
    "\n",
    "    STEPS_PER_EPOCH = 189/IMAGES_PER_GPU\n",
    "\n",
    "    VALIDATION_STEPS = 30\n",
    "    \n",
    "config = ViolinConfig()\n",
    "config.display()\n",
    "\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, message=re.escape(\"Input image dtype is bool. Interpolation is not defined with bool data type.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1919: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/mrcnn/model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/mrcnn/model.py:399: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/mrcnn/model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "WARNING:tensorflow:From /home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/mrcnn/model.py:723: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/mrcnn/model.py:725: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tani/anaconda3/envs/mrcnn/lib/python3.6/site-packages/mrcnn/model.py:775: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "Loading weights from  ./logs/violin20241111T0830/mask_rcnn_violin_0020.h5\n",
      "Re-starting from epoch 20\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(ViolinConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = './logs/violin20241111T0830/mask_rcnn_violin_0020.h5'\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViolinDataset(utils.Dataset):\n",
    "    def load_violins(self, dataset_dir, json_path):\n",
    "        # Ajouter la classe\n",
    "        self.add_class(\"violin\", 1, \"violin\")  # 1 est l'ID de la classe \"violon\"\n",
    "\n",
    "        # Charger les annotations depuis le fichier JSON\n",
    "        with open(json_path) as f:\n",
    "            annotations = json.load(f)\n",
    "        #print(annotations)\n",
    "        # Parcourir les annotations pour ajouter les images\n",
    "        for annotation in annotations:\n",
    "            #filename = annotation['filename']\n",
    "            filename = annotations[annotation]['filename']\n",
    "            #width = annotations[annotation]['width']\n",
    "            #height = annotations[annotation]['height']\n",
    "            size = annotations[annotation]['size']\n",
    "            polygons = annotations[annotation]['regions']  # Cela peut être des masques ou des boîtes englobantes\n",
    "\n",
    "            image_path = os.path.join(dataset_dir, filename)\n",
    "            self.add_image(\n",
    "                \"violin\",\n",
    "                image_id=filename,\n",
    "                path=image_path,\n",
    "                size=size,\n",
    "                polygons=polygons\n",
    "            )\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"\n",
    "        Charge les masques de segmentation pour une image donnée.\n",
    "        Chaque objet est représenté par un polygone (polyline).\n",
    "        \"\"\"\n",
    "        # Obtenir les informations de l'image à partir de image_id\n",
    "        info = self.image_info[image_id]\n",
    "        \n",
    "        # Charger l'image pour obtenir sa taille\n",
    "        image = self.load_image(image_id)\n",
    "        height, width = image.shape[:2]\n",
    "        \n",
    "        # Initialiser un masque vide de taille (hauteur, largeur, nombre de polygones)\n",
    "        mask = np.zeros((height, width, len(info['polygons'])), dtype=np.uint8)\n",
    "        # Liste des ID de classes pour les objets\n",
    "        class_ids = []\n",
    "        # Parcourir les régions annotées (polygones)\n",
    "        for i, region in enumerate(info['polygons']):\n",
    "            \n",
    "            # Extraire les coordonnées du polygone\n",
    "            shape_attr = region['shape_attributes']\n",
    "            all_points_x = shape_attr['all_points_x']\n",
    "            all_points_y = shape_attr['all_points_y']\n",
    "            # Convertir les coordonnées du polygone en indices de pixels\n",
    "            poly_x, poly_y = skimage.draw.polygon(all_points_y, all_points_x)\n",
    "            \n",
    "            # Assurer que les indices sont dans les limites de l'image\n",
    "            poly_x = np.clip(poly_x, 0, height - 1)\n",
    "            poly_y = np.clip(poly_y, 0, width - 1)\n",
    "            # Remplir le masque pour cette instance de polygone\n",
    "            mask[poly_x, poly_y, i] = 1\n",
    "            # Ajouter l'ID de la classe (par exemple, \"violin\")\n",
    "            class_ids.append(self.class_names.index(region['region_attributes']['type']))\n",
    "\n",
    "        # Retourner le masque et les ID des classes\n",
    "        return mask, np.array(class_ids)\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ViolinDataset()\n",
    "dataset_val.load_violins('./test', './test/via_project_10Nov2024_16h47m_json.json')\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP:  0.8455882352941176\n"
     ]
    }
   ],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on all validation images\n",
    "image_ids = dataset_val.image_ids\n",
    "APs = []\n",
    "\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask = \\\n",
    "        modellib.load_image_gt(dataset_val, inference_config, image_id, use_mini_mask=False)\n",
    "    \n",
    "    # Skip images without ground truth objects (e.g., no violins)\n",
    "    if len(gt_class_id) == 0:\n",
    "        continue  # Skip this image and move to the next one\n",
    "\n",
    "    # Preprocess the image for the model\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    \n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    \n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps = \\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    \n",
    "    APs.append(AP)\n",
    "\n",
    "# Compute the mean Average Precision (mAP)\n",
    "if len(APs) > 0:\n",
    "    mean_AP = np.mean(APs)\n",
    "    print(\"mAP: \", mean_AP)\n",
    "else:\n",
    "    print(\"No valid images with ground truth objects to calculate mAP.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP:  0.8455882352941176\n",
      "Precision:  0.9361702127659575\n",
      "Recall:  1.0\n",
      "F1-Score:  0.967032967032967\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive : 3\n"
     ]
    }
   ],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on all validation images\n",
    "image_ids = dataset_val.image_ids\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialiser les compteurs pour le calcul de la matrice de confusion\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "true_negatives = 0\n",
    "APs = []\n",
    "\n",
    "# Parcourir toutes les images de validation\n",
    "for image_id in image_ids:\n",
    "    # Charger l'image et les données de vérité terrain\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask = \\\n",
    "        modellib.load_image_gt(dataset_val, inference_config, image_id, use_mini_mask=False)\n",
    "\n",
    "    # Exécuter la détection d'objets\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "\n",
    "    # Vérifier si l'image contient des violons dans les données de vérité terrain\n",
    "    if len(gt_class_id) == 0:\n",
    "        # Cas où il n'y a pas de violon dans l'image (ground truth vide)\n",
    "        if len(r['class_ids']) > 0:\n",
    "            # Si le modèle détecte un violon alors qu'il n'y en a pas, c'est un faux positif\n",
    "            false_positives += len(r['class_ids'])\n",
    "        else:\n",
    "            # Si le modèle ne détecte aucun violon, c'est un vrai négatif\n",
    "            true_negatives += 1\n",
    "    else:\n",
    "        # Cas où il y a des violons dans l'image\n",
    "        # Calculer les AP pour ces images\n",
    "        AP, precisions, recalls, overlaps = \\\n",
    "            utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                             r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "        APs.append(AP)\n",
    "\n",
    "        # Calculer les True Positives, False Positives et False Negatives\n",
    "        for gt_id in gt_class_id:\n",
    "            if gt_id in r[\"class_ids\"]:\n",
    "                true_positives += 1  # Détection correcte\n",
    "            else:\n",
    "                false_negatives += 1  # Le modèle n'a pas détecté un violon présent\n",
    "\n",
    "        for pred_id in r[\"class_ids\"]:\n",
    "            if pred_id not in gt_class_id:\n",
    "                false_positives += 1  # Détection incorrecte\n",
    "\n",
    "# Calculer la moyenne des AP (mAP)\n",
    "if len(APs) > 0:\n",
    "    mean_AP = np.mean(APs)\n",
    "    print(\"mAP: \", mean_AP)\n",
    "else:\n",
    "    print(\"No valid images with ground truth objects to calculate mAP.\")\n",
    "\n",
    "# Calculer la précision, le rappel, la F1-score et les vrais négatifs\n",
    "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1-Score: \", f1)\n",
    "\n",
    "# Créer la matrice de confusion\n",
    "conf_matrix = np.array([\n",
    "    [true_negatives, false_positives],\n",
    "    [false_negatives, true_positives]\n",
    "])\n",
    "\n",
    "# Afficher la matrice de confusion\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Violin\", \"Violin\"], yticklabels=[\"No Violin\", \"Violin\"])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "print(\"False positive : \" + str(false_positives))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
