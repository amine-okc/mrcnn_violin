{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"./\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "import skimage.io\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           violin\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                20.0\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ViolinConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"violin\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # background + 1 shape\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 160/8\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ViolinConfig()\n",
    "config.display()\n",
    "\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, message=re.escape(\"Input image dtype is bool. Interpolation is not defined with bool data type.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from  ./logs/violin20241109T1129/mask_rcnn_violin_0050.h5\n",
      "Re-starting from epoch 50\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(ViolinConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = './logs/violin20241109T1129/mask_rcnn_violin_0050.h5'\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViolinDataset(utils.Dataset):\n",
    "    def load_violins(self, dataset_dir, json_path):\n",
    "        # Ajouter la classe\n",
    "        self.add_class(\"violin\", 1, \"violin\")  # 1 est l'ID de la classe \"violon\"\n",
    "\n",
    "        # Charger les annotations depuis le fichier JSON\n",
    "        with open(json_path) as f:\n",
    "            annotations = json.load(f)\n",
    "        #print(annotations)\n",
    "        # Parcourir les annotations pour ajouter les images\n",
    "        for annotation in annotations:\n",
    "            #filename = annotation['filename']\n",
    "            filename = annotations[annotation]['filename']\n",
    "            #width = annotations[annotation]['width']\n",
    "            #height = annotations[annotation]['height']\n",
    "            size = annotations[annotation]['size']\n",
    "            polygons = annotations[annotation]['regions']  # Cela peut être des masques ou des boîtes englobantes\n",
    "\n",
    "            image_path = os.path.join(dataset_dir, filename)\n",
    "            self.add_image(\n",
    "                \"violin\",\n",
    "                image_id=filename,\n",
    "                path=image_path,\n",
    "                size=size,\n",
    "                polygons=polygons\n",
    "            )\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"\n",
    "        Charge les masques de segmentation pour une image donnée.\n",
    "        Chaque objet est représenté par un polygone (polyline).\n",
    "        \"\"\"\n",
    "        # Obtenir les informations de l'image à partir de image_id\n",
    "        info = self.image_info[image_id]\n",
    "        \n",
    "        # Charger l'image pour obtenir sa taille\n",
    "        image = self.load_image(image_id)\n",
    "        height, width = image.shape[:2]\n",
    "        \n",
    "        # Initialiser un masque vide de taille (hauteur, largeur, nombre de polygones)\n",
    "        mask = np.zeros((height, width, len(info['polygons'])), dtype=np.uint8)\n",
    "        # Liste des ID de classes pour les objets\n",
    "        class_ids = []\n",
    "        # Parcourir les régions annotées (polygones)\n",
    "        for i, region in enumerate(info['polygons']):\n",
    "            \n",
    "            # Extraire les coordonnées du polygone\n",
    "            shape_attr = region['shape_attributes']\n",
    "            all_points_x = shape_attr['all_points_x']\n",
    "            all_points_y = shape_attr['all_points_y']\n",
    "            # Convertir les coordonnées du polygone en indices de pixels\n",
    "            poly_x, poly_y = skimage.draw.polygon(all_points_y, all_points_x)\n",
    "            \n",
    "            # Assurer que les indices sont dans les limites de l'image\n",
    "            poly_x = np.clip(poly_x, 0, height - 1)\n",
    "            poly_y = np.clip(poly_y, 0, width - 1)\n",
    "            # Remplir le masque pour cette instance de polygone\n",
    "            mask[poly_x, poly_y, i] = 1\n",
    "            # Ajouter l'ID de la classe (par exemple, \"violin\")\n",
    "            class_ids.append(self.class_names.index(region['region_attributes']['type']))\n",
    "\n",
    "        # Retourner le masque et les ID des classes\n",
    "        return mask, np.array(class_ids)\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ViolinDataset()\n",
    "dataset_val.load_violins('./test', './test/via_project_8Nov2024_22h27m_json.json')\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute VOC-Style mAP @ IoU=0.5\n",
    "# # Running on all validation images\n",
    "# image_ids = dataset_val.image_ids\n",
    "# APs = []\n",
    "\n",
    "# for image_id in image_ids:\n",
    "#     # Load image and ground truth data\n",
    "#     image, image_meta, gt_class_id, gt_bbox, gt_mask = \\\n",
    "#         modellib.load_image_gt(dataset_val, inference_config, image_id, use_mini_mask=False)\n",
    "    \n",
    "#     # Skip images without ground truth objects (e.g., no violins)\n",
    "#     if len(gt_class_id) == 0:\n",
    "#         continue  # Skip this image and move to the next one\n",
    "\n",
    "#     # Preprocess the image for the model\n",
    "#     molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    \n",
    "#     # Run object detection\n",
    "#     results = model.detect([image], verbose=0)\n",
    "#     r = results[0]\n",
    "    \n",
    "#     # Compute AP\n",
    "#     AP, precisions, recalls, overlaps = \\\n",
    "#         utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "#                          r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    \n",
    "#     APs.append(AP)\n",
    "\n",
    "# # Compute the mean Average Precision (mAP)\n",
    "# if len(APs) > 0:\n",
    "#     mean_AP = np.mean(APs)\n",
    "#     print(\"mAP: \", mean_AP)\n",
    "# else:\n",
    "#     print(\"No valid images with ground truth objects to calculate mAP.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP:  0.7323232323232324\n",
      "Precision:  0.9347826086956522\n",
      "Recall:  1.0\n",
      "F1-Score:  0.9662921348314606\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive : 3\n"
     ]
    }
   ],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on all validation images\n",
    "image_ids = dataset_val.image_ids\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialiser les compteurs pour le calcul de la matrice de confusion\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "true_negatives = 0\n",
    "APs = []\n",
    "\n",
    "# Parcourir toutes les images de validation\n",
    "for image_id in image_ids:\n",
    "    # Charger l'image et les données de vérité terrain\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask = \\\n",
    "        modellib.load_image_gt(dataset_val, inference_config, image_id, use_mini_mask=False)\n",
    "\n",
    "    # Exécuter la détection d'objets\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "\n",
    "    # Vérifier si l'image contient des violons dans les données de vérité terrain\n",
    "    if len(gt_class_id) == 0:\n",
    "        # Cas où il n'y a pas de violon dans l'image (ground truth vide)\n",
    "        if len(r['class_ids']) > 0:\n",
    "            # Si le modèle détecte un violon alors qu'il n'y en a pas, c'est un faux positif\n",
    "            false_positives += len(r['class_ids'])\n",
    "        else:\n",
    "            # Si le modèle ne détecte aucun violon, c'est un vrai négatif\n",
    "            true_negatives += 1\n",
    "    else:\n",
    "        # Cas où il y a des violons dans l'image\n",
    "        # Calculer les AP pour ces images\n",
    "        AP, precisions, recalls, overlaps = \\\n",
    "            utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                             r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "        APs.append(AP)\n",
    "\n",
    "        # Calculer les True Positives, False Positives et False Negatives\n",
    "        for gt_id in gt_class_id:\n",
    "            if gt_id in r[\"class_ids\"]:\n",
    "                true_positives += 1  # Détection correcte\n",
    "            else:\n",
    "                false_negatives += 1  # Le modèle n'a pas détecté un violon présent\n",
    "\n",
    "        for pred_id in r[\"class_ids\"]:\n",
    "            if pred_id not in gt_class_id:\n",
    "                false_positives += 1  # Détection incorrecte\n",
    "\n",
    "# Calculer la moyenne des AP (mAP)\n",
    "if len(APs) > 0:\n",
    "    mean_AP = np.mean(APs)\n",
    "    print(\"mAP: \", mean_AP)\n",
    "else:\n",
    "    print(\"No valid images with ground truth objects to calculate mAP.\")\n",
    "\n",
    "# Calculer la précision, le rappel, la F1-score et les vrais négatifs\n",
    "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1-Score: \", f1)\n",
    "\n",
    "# Créer la matrice de confusion\n",
    "conf_matrix = np.array([\n",
    "    [true_negatives, false_positives],\n",
    "    [false_negatives, true_positives]\n",
    "])\n",
    "\n",
    "# Afficher la matrice de confusion\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Violin\", \"Violin\"], yticklabels=[\"No Violin\", \"Violin\"])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
